{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Custom Dataset class for MNIST\n",
    "class MNISTDataset(Dataset):\n",
    "    def __init__(self, csv_file):\n",
    "        # Read CSV file\n",
    "        data = pd.read_csv(csv_file)\n",
    "\n",
    "        # Separate labels and features\n",
    "        self.labels = data.iloc[:, 0].values\n",
    "        self.features = data.iloc[:, 1:].values\n",
    "\n",
    "        # First normalize to [0,1]\n",
    "        self.features = self.features / 255.0\n",
    "\n",
    "        # Then center the data\n",
    "        self.features = self.features - np.mean(self.features, axis=0)\n",
    "\n",
    "        # Rescale to [0,1] after centering\n",
    "        min_vals = np.min(self.features, axis=0)\n",
    "        max_vals = np.max(self.features, axis=0)\n",
    "        self.features = (self.features - min_vals) / (max_vals - min_vals + 1e-8)\n",
    "\n",
    "        # Convert to tensors\n",
    "        self.features = torch.FloatTensor(self.features)\n",
    "        self.labels = torch.LongTensor(self.labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n",
    "\n",
    "\n",
    "# Regular Autoencoder with 3 layers\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_size=784, latent_size=128):\n",
    "        super(Autoencoder, self).__init__()\n",
    "\n",
    "        # Define intermediate layer sizes\n",
    "        h1_size = 512  # First hidden layer\n",
    "        h2_size = 256  # Second hidden layer\n",
    "\n",
    "        # Encoder layers\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, h1_size, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(h1_size, h2_size, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(h2_size, latent_size, bias=False)\n",
    "        )\n",
    "\n",
    "        # Decoder layers\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_size, h2_size, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(h2_size, h1_size, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(h1_size, input_size, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        # Initialize weights\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "    def count_parameters(self):\n",
    "        \"\"\"Count the number of trainable parameters\"\"\"\n",
    "        encoder_params = sum(p.numel() for p in self.encoder.parameters() if p.requires_grad)\n",
    "        decoder_params = sum(p.numel() for p in self.decoder.parameters() if p.requires_grad)\n",
    "        total_params = encoder_params + decoder_params\n",
    "\n",
    "        print(\"\\nRegular Autoencoder Parameters:\")\n",
    "        print(f\"Encoder parameters: {encoder_params:,}\")\n",
    "        print(f\"Decoder parameters: {decoder_params:,}\")\n",
    "        print(f\"Total parameters: {total_params:,}\")\n",
    "        return total_params\n",
    "\n",
    "\n",
    "# Tied Weights Autoencoder\n",
    "class TiedAutoencoder(nn.Module):\n",
    "    def __init__(self, input_size=784, latent_size=128):\n",
    "        super(TiedAutoencoder, self).__init__()\n",
    "\n",
    "        # Define intermediate layer sizes\n",
    "        h1_size = 512  # First hidden layer\n",
    "        h2_size = 256  # Second hidden layer\n",
    "\n",
    "        # Encoder layers - only store encoder weights\n",
    "        self.encoder_layer1 = nn.Linear(input_size, h1_size, bias=False)\n",
    "        self.encoder_layer2 = nn.Linear(h1_size, h2_size, bias=False)\n",
    "        self.encoder_layer3 = nn.Linear(h2_size, latent_size, bias=False)\n",
    "\n",
    "        # Encoder activation functions\n",
    "        self.encoder_activation = nn.LeakyReLU(negative_slope=0.2)\n",
    "\n",
    "        # Decoder activation functions (inverse of LeakyReLU)\n",
    "        self.decoder_activation = lambda x: torch.where(x < 0, x / 0.2, x)\n",
    "\n",
    "        # Initialize weights\n",
    "        nn.init.xavier_uniform_(self.encoder_layer1.weight)\n",
    "        nn.init.xavier_uniform_(self.encoder_layer2.weight)\n",
    "        nn.init.xavier_uniform_(self.encoder_layer3.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Flatten input\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        # Encode\n",
    "        e1 = self.encoder_activation(self.encoder_layer1(x))\n",
    "        e2 = self.encoder_activation(self.encoder_layer2(e1))\n",
    "        encoded = self.encoder_layer3(e2)\n",
    "\n",
    "        # Decode using transposed weights\n",
    "        d1 = self.decoder_activation(F.linear(encoded, self.encoder_layer3.weight.t()))\n",
    "        d2 = self.decoder_activation(F.linear(d1, self.encoder_layer2.weight.t()))\n",
    "        decoded = torch.sigmoid(F.linear(d2, self.encoder_layer1.weight.t()))\n",
    "\n",
    "        return decoded\n",
    "\n",
    "    def count_parameters(self):\n",
    "        \"\"\"Count the number of trainable parameters\"\"\"\n",
    "        params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "        print(\"\\nTied Weights Autoencoder Parameters:\")\n",
    "        print(f\"Encoder parameters (only stored weights): {params:,}\")\n",
    "        print(f\"Decoder parameters: 0 (uses transposed encoder weights)\")\n",
    "        print(f\"Total unique parameters: {params:,}\")\n",
    "        return params\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, num_epochs=40, learning_rate=0.001):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    train_losses = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for data, _ in train_loader:\n",
    "            data = data.to(device)\n",
    "\n",
    "            output = model(data)\n",
    "            loss = criterion(output, data.view(data.size(0), -1))\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        train_losses.append(epoch_loss)\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {epoch_loss:.6f}')\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_losses)\n",
    "    plt.title('Training Loss Over Time')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def visualize_reconstructions(model, test_loader, num_images=5):\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    with torch.no_grad():\n",
    "        data, _ = next(iter(test_loader))\n",
    "        data = data[:num_images].to(device)\n",
    "        reconstructions = model(data)\n",
    "\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        for i in range(num_images):\n",
    "            # Original\n",
    "            plt.subplot(2, num_images, i + 1)\n",
    "            plt.imshow(data[i].cpu().view(28, 28), cmap='gray')\n",
    "            plt.title('Original')\n",
    "            plt.axis('off')\n",
    "\n",
    "            # Reconstruction\n",
    "            plt.subplot(2, num_images, i + num_images + 1)\n",
    "            plt.imshow(reconstructions[i].cpu().view(28, 28), cmap='gray')\n",
    "            plt.title('Reconstructed')\n",
    "            plt.axis('off')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def explain_architecture():\n",
    "    print(\"\\nArchitecture Explanation:\")\n",
    "    print(\"1. Weight Tying in TiedAutoencoder:\")\n",
    "    print(\"   - Decoder weights are transpose of encoder weights\")\n",
    "    print(\"   - Reduces parameters by half\")\n",
    "    print(\"   - Follows PCA's reconstruction principle\")\n",
    "\n",
    "    print(\"\\n2. Activation Functions:\")\n",
    "    print(\"   - Encoder: LeakyReLU with slope 0.2\")\n",
    "    print(\"   - Decoder: Inverse LeakyReLU with slope 1/0.2 = 5\")\n",
    "    print(\"   - Ensures proper reconstruction through inverse transformations\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Hyperparameters\n",
    "    BATCH_SIZE = 250\n",
    "    LATENT_SIZE = 128\n",
    "    NUM_EPOCHS = 40\n",
    "\n",
    "    # Load datasets\n",
    "    print(\"Loading datasets...\")\n",
    "    train_dataset = MNISTDataset('mnist_train.csv')\n",
    "    test_dataset = MNISTDataset('mnist_test.csv')\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    print(\"Datasets loaded successfully\")\n",
    "\n",
    "    # Test both architectures\n",
    "    print(\"\\nTesting Regular Autoencoder:\")\n",
    "    regular_ae = Autoencoder(input_size=784, latent_size=LATENT_SIZE)\n",
    "    regular_ae.count_parameters()\n",
    "\n",
    "    print(\"\\nTesting Tied Weights Autoencoder:\")\n",
    "    tied_ae = TiedAutoencoder(input_size=784, latent_size=LATENT_SIZE)\n",
    "    tied_ae.count_parameters()\n",
    "\n",
    "    explain_architecture()\n",
    "\n",
    "    # Train the tied weights version\n",
    "    print(\"\\nTraining Tied Weights Autoencoder...\")\n",
    "    train_model(tied_ae, train_loader, NUM_EPOCHS)\n",
    "\n",
    "    # Visualize results\n",
    "    print(\"Generating visualizations...\")\n",
    "    visualize_reconstructions(tied_ae, test_loader)\n",
    "\n",
    "    print(\"Training completed!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Custom Dataset class for MNIST\n",
    "class MNISTDataset(Dataset):\n",
    "    def __init__(self, csv_file):\n",
    "        # Read CSV file\n",
    "        data = pd.read_csv(csv_file)\n",
    "\n",
    "        # Separate labels and features\n",
    "        self.labels = data.iloc[:, 0].values\n",
    "        self.features = data.iloc[:, 1:].values\n",
    "\n",
    "        # First normalize to [0,1]\n",
    "        self.features = self.features / 255.0\n",
    "\n",
    "        # Then center the data\n",
    "        self.features = self.features - np.mean(self.features, axis=0)\n",
    "\n",
    "        # Rescale to [0,1] after centering\n",
    "        min_vals = np.min(self.features, axis=0)\n",
    "        max_vals = np.max(self.features, axis=0)\n",
    "        self.features = (self.features - min_vals) / (max_vals - min_vals + 1e-8)\n",
    "\n",
    "        # Convert to tensors\n",
    "        self.features = torch.FloatTensor(self.features)\n",
    "        self.labels = torch.LongTensor(self.labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n",
    "\n",
    "\n",
    "# Regular Autoencoder with 3 layers\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_size=784, latent_size=128):\n",
    "        super(Autoencoder, self).__init__()\n",
    "\n",
    "        # Define intermediate layer sizes\n",
    "        h1_size = 512  # First hidden layer\n",
    "        h2_size = 256  # Second hidden layer\n",
    "\n",
    "        # Encoder layers\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, h1_size, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(h1_size, h2_size, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(h2_size, latent_size, bias=False)\n",
    "        )\n",
    "\n",
    "        # Decoder layers\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_size, h2_size, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(h2_size, h1_size, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(h1_size, input_size, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        # Initialize weights\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "    def count_parameters(self):\n",
    "        \"\"\"Count the number of trainable parameters\"\"\"\n",
    "        encoder_params = sum(p.numel() for p in self.encoder.parameters() if p.requires_grad)\n",
    "        decoder_params = sum(p.numel() for p in self.decoder.parameters() if p.requires_grad)\n",
    "        total_params = encoder_params + decoder_params\n",
    "\n",
    "        print(\"\\nRegular Autoencoder Parameters:\")\n",
    "        print(f\"Encoder parameters: {encoder_params:,}\")\n",
    "        print(f\"Decoder parameters: {decoder_params:,}\")\n",
    "        print(f\"Total parameters: {total_params:,}\")\n",
    "        return total_params\n",
    "\n",
    "\n",
    "# Tied Weights Autoencoder\n",
    "class TiedAutoencoder(nn.Module):\n",
    "    def __init__(self, input_size=784, latent_size=128):\n",
    "        super(TiedAutoencoder, self).__init__()\n",
    "\n",
    "        # Define intermediate layer sizes\n",
    "        h1_size = 512  # First hidden layer\n",
    "        h2_size = 256  # Second hidden layer\n",
    "\n",
    "        # Encoder layers - only store encoder weights\n",
    "        self.encoder_layer1 = nn.Linear(input_size, h1_size, bias=False)\n",
    "        self.encoder_layer2 = nn.Linear(h1_size, h2_size, bias=False)\n",
    "        self.encoder_layer3 = nn.Linear(h2_size, latent_size, bias=False)\n",
    "\n",
    "        # Encoder activation functions\n",
    "        self.encoder_activation = nn.LeakyReLU(negative_slope=0.2)\n",
    "\n",
    "        # Decoder activation functions (inverse of LeakyReLU)\n",
    "        self.decoder_activation = lambda x: torch.where(x < 0, x / 0.2, x)\n",
    "\n",
    "        # Initialize weights\n",
    "        nn.init.xavier_uniform_(self.encoder_layer1.weight)\n",
    "        nn.init.xavier_uniform_(self.encoder_layer2.weight)\n",
    "        nn.init.xavier_uniform_(self.encoder_layer3.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Flatten input\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        # Encode\n",
    "        e1 = self.encoder_activation(self.encoder_layer1(x))\n",
    "        e2 = self.encoder_activation(self.encoder_layer2(e1))\n",
    "        encoded = self.encoder_layer3(e2)\n",
    "\n",
    "        # Decode using transposed weights\n",
    "        d1 = self.decoder_activation(F.linear(encoded, self.encoder_layer3.weight.t()))\n",
    "        d2 = self.decoder_activation(F.linear(d1, self.encoder_layer2.weight.t()))\n",
    "        decoded = torch.sigmoid(F.linear(d2, self.encoder_layer1.weight.t()))\n",
    "\n",
    "        return decoded\n",
    "\n",
    "    def count_parameters(self):\n",
    "        \"\"\"Count the number of trainable parameters\"\"\"\n",
    "        params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "        print(\"\\nTied Weights Autoencoder Parameters:\")\n",
    "        print(f\"Encoder parameters (only stored weights): {params:,}\")\n",
    "        print(f\"Decoder parameters: 0 (uses transposed encoder weights)\")\n",
    "        print(f\"Total unique parameters: {params:,}\")\n",
    "        return params\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, num_epochs=40, learning_rate=0.001):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    train_losses = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for data, _ in train_loader:\n",
    "            data = data.to(device)\n",
    "\n",
    "            output = model(data)\n",
    "            loss = criterion(output, data.view(data.size(0), -1))\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        train_losses.append(epoch_loss)\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {epoch_loss:.6f}')\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_losses)\n",
    "    plt.title('Training Loss Over Time')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def visualize_reconstructions(model, test_loader, num_images=5):\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    with torch.no_grad():\n",
    "        data, _ = next(iter(test_loader))\n",
    "        data = data[:num_images].to(device)\n",
    "        reconstructions = model(data)\n",
    "\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        for i in range(num_images):\n",
    "            # Original\n",
    "            plt.subplot(2, num_images, i + 1)\n",
    "            plt.imshow(data[i].cpu().view(28, 28), cmap='gray')\n",
    "            plt.title('Original')\n",
    "            plt.axis('off')\n",
    "\n",
    "            # Reconstruction\n",
    "            plt.subplot(2, num_images, i + num_images + 1)\n",
    "            plt.imshow(reconstructions[i].cpu().view(28, 28), cmap='gray')\n",
    "            plt.title('Reconstructed')\n",
    "            plt.axis('off')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def explain_architecture():\n",
    "    print(\"\\nArchitecture Explanation:\")\n",
    "    print(\"1. Weight Tying in TiedAutoencoder:\")\n",
    "    print(\"   - Decoder weights are transpose of encoder weights\")\n",
    "    print(\"   - Reduces parameters by half\")\n",
    "    print(\"   - Follows PCA's reconstruction principle\")\n",
    "\n",
    "    print(\"\\n2. Activation Functions:\")\n",
    "    print(\"   - Encoder: LeakyReLU with slope 0.2\")\n",
    "    print(\"   - Decoder: Inverse LeakyReLU with slope 1/0.2 = 5\")\n",
    "    print(\"   - Ensures proper reconstruction through inverse transformations\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Hyperparameters\n",
    "    BATCH_SIZE = 250\n",
    "    LATENT_SIZE = 128\n",
    "    NUM_EPOCHS = 40\n",
    "\n",
    "    # Load datasets\n",
    "    print(\"Loading datasets...\")\n",
    "    train_dataset = MNISTDataset('mnist_train.csv')\n",
    "    test_dataset = MNISTDataset('mnist_test.csv')\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    print(\"Datasets loaded successfully\")\n",
    "\n",
    "    # Test both architectures\n",
    "    print(\"\\nTesting Regular Autoencoder:\")\n",
    "    regular_ae = Autoencoder(input_size=784, latent_size=LATENT_SIZE)\n",
    "    regular_ae.count_parameters()\n",
    "\n",
    "    print(\"\\nTesting Tied Weights Autoencoder:\")\n",
    "    tied_ae = TiedAutoencoder(input_size=784, latent_size=LATENT_SIZE)\n",
    "    tied_ae.count_parameters()\n",
    "\n",
    "    explain_architecture()\n",
    "\n",
    "    # Train the tied weights version\n",
    "    print(\"\\nTraining Tied Weights Autoencoder...\")\n",
    "    train_model(tied_ae, train_loader, NUM_EPOCHS)\n",
    "\n",
    "    # Visualize results\n",
    "    print(\"Generating visualizations...\")\n",
    "    visualize_reconstructions(tied_ae, test_loader)\n",
    "\n",
    "    print(\"Training completed!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
