{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Custom Dataset class for MNIST\n",
    "class MNISTDataset(Dataset):\n",
    "    def __init__(self, csv_file):\n",
    "        # Read CSV file\n",
    "        data = pd.read_csv(csv_file)\n",
    "\n",
    "        # Separate labels and features\n",
    "        self.labels = data.iloc[:, 0].values\n",
    "        self.features = data.iloc[:, 1:].values\n",
    "\n",
    "        # First normalize to [0,1]\n",
    "        self.features = self.features / 255.0\n",
    "\n",
    "        # Then center the data\n",
    "        self.features = self.features - np.mean(self.features, axis=0)\n",
    "\n",
    "        # Rescale to [0,1] after centering\n",
    "        min_vals = np.min(self.features, axis=0)\n",
    "        max_vals = np.max(self.features, axis=0)\n",
    "        self.features = (self.features - min_vals) / (max_vals - min_vals + 1e-8)\n",
    "\n",
    "        # Convert to tensors\n",
    "        self.features = torch.FloatTensor(self.features)\n",
    "        self.labels = torch.LongTensor(self.labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n",
    "\n",
    "\n",
    "# Autoencoder Model\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_size=784, latent_size=128):\n",
    "        super(Autoencoder, self).__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = nn.Linear(input_size, latent_size, bias=False)\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = nn.Linear(latent_size, input_size, bias=False)\n",
    "\n",
    "        # Initialize weights\n",
    "        nn.init.xavier_uniform_(self.encoder.weight)\n",
    "        nn.init.xavier_uniform_(self.decoder.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Flatten input\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        # Encode\n",
    "        encoded = self.encoder(x)\n",
    "\n",
    "        # Decode\n",
    "        decoded = torch.sigmoid(self.decoder(encoded))\n",
    "\n",
    "        return decoded\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, num_epochs=40, learning_rate=0.001):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Lists to store losses\n",
    "    train_losses = []\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for data, _ in train_loader:\n",
    "            data = data.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            output = model(data)\n",
    "            loss = criterion(output, data.view(data.size(0), -1))\n",
    "\n",
    "            # Backward pass and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        train_losses.append(epoch_loss)\n",
    "\n",
    "        # Print epoch statistics\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {epoch_loss:.6f}')\n",
    "\n",
    "        # Compare encoder weights with PCA matrix\n",
    "        if epoch % 10 == 0:\n",
    "            compare_weights_with_pca(model.encoder.weight.data)\n",
    "\n",
    "    # Plot training loss\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_losses)\n",
    "    plt.title('Training Loss Over Time')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def compare_weights_with_pca(encoder_weights):\n",
    "    # Here you would compare with VL matrix from PCA\n",
    "    print(f\"Encoder weights shape: {encoder_weights.shape}\")\n",
    "\n",
    "\n",
    "def visualize_reconstructions(model, test_loader, num_images=5):\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Get a batch of test data\n",
    "        data, _ = next(iter(test_loader))\n",
    "        data = data[:num_images].to(device)\n",
    "\n",
    "        # Get reconstructions\n",
    "        reconstructions = model(data)\n",
    "\n",
    "        # Plot original and reconstructed images\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        for i in range(num_images):\n",
    "            # Original\n",
    "            plt.subplot(2, num_images, i + 1)\n",
    "            plt.imshow(data[i].cpu().view(28, 28), cmap='gray')\n",
    "            plt.title('Original')\n",
    "            plt.axis('off')\n",
    "\n",
    "            # Reconstruction\n",
    "            plt.subplot(2, num_images, i + num_images + 1)\n",
    "            plt.imshow(reconstructions[i].cpu().view(28, 28), cmap='gray')\n",
    "            plt.title('Reconstructed')\n",
    "            plt.axis('off')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Hyperparameters\n",
    "    BATCH_SIZE = 250\n",
    "    LATENT_SIZE = 128\n",
    "    NUM_EPOCHS = 40\n",
    "\n",
    "    # Load datasets\n",
    "    print(\"Loading datasets...\")\n",
    "    train_dataset = MNISTDataset('mnist_train.csv')\n",
    "    test_dataset = MNISTDataset('mnist_test.csv')\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    print(\"Datasets loaded successfully\")\n",
    "\n",
    "    # Initialize model\n",
    "    print(\"Initializing model...\")\n",
    "    model = Autoencoder(input_size=784, latent_size=LATENT_SIZE)\n",
    "\n",
    "    # Train model\n",
    "    print(\"Starting training...\")\n",
    "    train_model(model, train_loader, NUM_EPOCHS)\n",
    "\n",
    "    # Visualize results\n",
    "    print(\"Generating visualizations...\")\n",
    "    visualize_reconstructions(model, test_loader)\n",
    "\n",
    "    print(\"Training completed!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
