{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "\n",
    "# Dataset class\n",
    "class MNISTDataset(Dataset):\n",
    "    def __init__(self, csv_file):\n",
    "        data = pd.read_csv(csv_file)\n",
    "        self.labels = data.iloc[:, 0].values\n",
    "        self.features = data.iloc[:, 1:].values\n",
    "        self.features = self.features / 255.0\n",
    "        self.features = self.features - np.mean(self.features, axis=0)\n",
    "        min_vals = np.min(self.features, axis=0)\n",
    "        max_vals = np.max(self.features, axis=0)\n",
    "        self.features = (self.features - min_vals) / (max_vals - min_vals + 1e-8)\n",
    "        self.features = torch.FloatTensor(self.features)\n",
    "        self.labels = torch.LongTensor(self.labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n",
    "\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, num_epochs=40, learning_rate=0.001):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for data, _ in train_loader:\n",
    "            data = data.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, data.view(data.size(0), -1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(train_loader):.6f}')\n",
    "\n",
    "\n",
    "def evaluate_model(model: nn.Module, test_loader: DataLoader, device: torch.device) -> float:\n",
    "    \"\"\"Calculate MSE reconstruction error on test dataset\"\"\"\n",
    "    model.eval()\n",
    "    total_mse = 0.0\n",
    "    num_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, _ in test_loader:\n",
    "            data = data.to(device)\n",
    "            reconstructed = model(data)\n",
    "            mse = F.mse_loss(reconstructed, data.view(data.size(0), -1))\n",
    "            total_mse += mse.item() * data.size(0)\n",
    "            num_samples += data.size(0)\n",
    "\n",
    "    return total_mse / num_samples\n",
    "\n",
    "\n",
    "# Import the three autoencoder variants\n",
    "from autoencoder_mnist_3 import Autoencoder as ThreeLayerAutoencoder\n",
    "from autoencoder_mnist_4 import PseudoinverseAutoencoder\n",
    "from autoencoder_mnist_3 import TiedAutoencoder  # This should be in your autoencoder_mnist_3.py file\n",
    "\n",
    "\n",
    "def compare_autoencoders(train_loader: DataLoader, test_loader: DataLoader) -> Tuple[Dict[str, float], plt.Figure]:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Initialize models\n",
    "    models = {\n",
    "        'Three Layer AE': ThreeLayerAutoencoder(input_size=784, latent_size=128),\n",
    "        'Tied Weights AE': TiedAutoencoder(input_size=784, latent_size=128),\n",
    "        'Pseudoinverse AE': PseudoinverseAutoencoder(input_size=784, latent_size=128)\n",
    "    }\n",
    "\n",
    "    # Move models to device\n",
    "    for model in models.values():\n",
    "        model.to(device)\n",
    "\n",
    "    # Train models and collect MSE scores\n",
    "    mse_scores = {}\n",
    "    for name, model in models.items():\n",
    "        print(f\"\\nTraining {name}...\")\n",
    "        train_model(model, train_loader)\n",
    "        mse = evaluate_model(model, test_loader, device)\n",
    "        mse_scores[name] = mse\n",
    "\n",
    "    # Create visualization comparing reconstructions\n",
    "    fig = plt.figure(figsize=(15, 12))\n",
    "    with torch.no_grad():\n",
    "        data, _ = next(iter(test_loader))\n",
    "        data = data[:5].to(device)\n",
    "\n",
    "        # Plot original images in first row\n",
    "        for i in range(5):\n",
    "            plt.subplot(4, 5, i + 1)\n",
    "            plt.imshow(data[i].cpu().view(28, 28), cmap='gray')\n",
    "            if i == 0:\n",
    "                plt.title('Original', pad=20)\n",
    "            plt.axis('off')\n",
    "\n",
    "        # Plot reconstructions for each model\n",
    "        for row, (name, model) in enumerate(models.items(), start=1):\n",
    "            reconstructed = model(data)\n",
    "            for i in range(5):\n",
    "                plt.subplot(4, 5, row * 5 + i + 1)\n",
    "                plt.imshow(reconstructed[i].cpu().view(28, 28), cmap='gray')\n",
    "                if i == 0:\n",
    "                    plt.title(f'{name}\\nMSE: {mse_scores[name]:.6f}', pad=20)\n",
    "                plt.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return mse_scores, fig\n",
    "\n",
    "\n",
    "def print_mse_table(mse_scores: Dict[str, float]):\n",
    "    \"\"\"Print MSE scores in a formatted table\"\"\"\n",
    "    print(\"\\nReconstruction MSE Comparison:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"{'Model':<25} {'MSE':<15}\")\n",
    "    print(\"-\" * 50)\n",
    "    for name, mse in mse_scores.items():\n",
    "        print(f\"{name:<25} {mse:<15.6f}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Load datasets\n",
    "    print(\"Loading datasets...\")\n",
    "    train_dataset = MNISTDataset('mnist_train.csv')\n",
    "    test_dataset = MNISTDataset('mnist_test.csv')\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=250, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=250, shuffle=False)\n",
    "    print(\"Datasets loaded successfully\")\n",
    "\n",
    "    # Compare models\n",
    "    print(\"\\nComparing autoencoder variants...\")\n",
    "    mse_scores, fig = compare_autoencoders(train_loader, test_loader)\n",
    "\n",
    "    # Print results table\n",
    "    print_mse_table(mse_scores)\n",
    "\n",
    "    # Show comparison plot\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
