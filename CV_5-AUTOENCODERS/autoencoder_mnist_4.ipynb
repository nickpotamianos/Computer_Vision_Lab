{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Custom Dataset class for MNIST (same as before)\n",
    "class MNISTDataset(Dataset):\n",
    "    def __init__(self, csv_file):\n",
    "        data = pd.read_csv(csv_file)\n",
    "        self.labels = data.iloc[:, 0].values\n",
    "        self.features = data.iloc[:, 1:].values\n",
    "        self.features = self.features / 255.0\n",
    "        self.features = self.features - np.mean(self.features, axis=0)\n",
    "        min_vals = np.min(self.features, axis=0)\n",
    "        max_vals = np.max(self.features, axis=0)\n",
    "        self.features = (self.features - min_vals) / (max_vals - min_vals + 1e-8)\n",
    "        self.features = torch.FloatTensor(self.features)\n",
    "        self.labels = torch.LongTensor(self.labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n",
    "\n",
    "\n",
    "# Pseudoinverse Autoencoder\n",
    "class PseudoinverseAutoencoder(nn.Module):\n",
    "    def __init__(self, input_size=784, latent_size=128):\n",
    "        super(PseudoinverseAutoencoder, self).__init__()\n",
    "\n",
    "        # Define intermediate layer sizes\n",
    "        h1_size = 512  # First hidden layer\n",
    "        h2_size = 256  # Second hidden layer\n",
    "\n",
    "        # Encoder layers - only store encoder weights\n",
    "        self.encoder_layer1 = nn.Linear(input_size, h1_size, bias=False)\n",
    "        self.encoder_layer2 = nn.Linear(h1_size, h2_size, bias=False)\n",
    "        self.encoder_layer3 = nn.Linear(h2_size, latent_size, bias=False)\n",
    "\n",
    "        # Encoder activation functions\n",
    "        self.encoder_activation = nn.LeakyReLU(negative_slope=0.2)\n",
    "\n",
    "        # Decoder activation functions (inverse of LeakyReLU)\n",
    "        self.decoder_activation = lambda x: torch.where(x < 0, x / 0.2, x)\n",
    "\n",
    "        # Initialize weights\n",
    "        nn.init.xavier_uniform_(self.encoder_layer1.weight)\n",
    "        nn.init.xavier_uniform_(self.encoder_layer2.weight)\n",
    "        nn.init.xavier_uniform_(self.encoder_layer3.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Flatten input\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        # Encode\n",
    "        e1 = self.encoder_activation(self.encoder_layer1(x))\n",
    "        e2 = self.encoder_activation(self.encoder_layer2(e1))\n",
    "        encoded = self.encoder_layer3(e2)\n",
    "\n",
    "        # Decode using pseudoinverse weights\n",
    "        # Note: pinverse is computationally expensive, in practice you might want to cache these\n",
    "        d1 = self.decoder_activation(F.linear(encoded, torch.pinverse(self.encoder_layer3.weight)))\n",
    "        d2 = self.decoder_activation(F.linear(d1, torch.pinverse(self.encoder_layer2.weight)))\n",
    "        decoded = torch.sigmoid(F.linear(d2, torch.pinverse(self.encoder_layer1.weight)))\n",
    "\n",
    "        return decoded\n",
    "\n",
    "    def count_parameters(self):\n",
    "        \"\"\"Count the number of trainable parameters\"\"\"\n",
    "        params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "        print(\"\\nPseudoinverse Autoencoder Parameters:\")\n",
    "        print(f\"Encoder parameters (only stored weights): {params:,}\")\n",
    "        print(f\"Decoder parameters: 0 (uses pseudoinverse of encoder weights)\")\n",
    "        print(f\"Total unique parameters: {params:,}\")\n",
    "        return params\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, num_epochs=40, learning_rate=0.001):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    train_losses = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for data, _ in train_loader:\n",
    "            data = data.to(device)\n",
    "\n",
    "            output = model(data)\n",
    "            loss = criterion(output, data.view(data.size(0), -1))\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        train_losses.append(epoch_loss)\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {epoch_loss:.6f}')\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_losses)\n",
    "    plt.title('Training Loss Over Time')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def visualize_reconstructions(model, test_loader, num_images=5):\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    with torch.no_grad():\n",
    "        data, _ = next(iter(test_loader))\n",
    "        data = data[:num_images].to(device)\n",
    "        reconstructions = model(data)\n",
    "\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        for i in range(num_images):\n",
    "            plt.subplot(2, num_images, i + 1)\n",
    "            plt.imshow(data[i].cpu().view(28, 28), cmap='gray')\n",
    "            plt.title('Original')\n",
    "            plt.axis('off')\n",
    "\n",
    "            plt.subplot(2, num_images, i + num_images + 1)\n",
    "            plt.imshow(reconstructions[i].cpu().view(28, 28), cmap='gray')\n",
    "            plt.title('Reconstructed')\n",
    "            plt.axis('off')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def explain_pseudoinverse_architecture():\n",
    "    print(\"\\nPseudoinverse Autoencoder Architecture Explanation:\")\n",
    "    print(\"1. Weight Relationship:\")\n",
    "    print(\"   - Decoder weights are pseudoinverse of encoder weights\")\n",
    "    print(\"   - Uses Moore-Penrose pseudoinverse for better reconstruction\")\n",
    "    print(\"   - Handles cases where regular inverse might not exist\")\n",
    "\n",
    "    print(\"\\n2. Activation Functions:\")\n",
    "    print(\"   - Encoder: LeakyReLU with slope 0.2\")\n",
    "    print(\"   - Decoder: Inverse LeakyReLU with slope 1/0.2 = 5\")\n",
    "    print(\"   - Maintains reconstruction capability through inverse functions\")\n",
    "\n",
    "    print(\"\\n3. Advantages:\")\n",
    "    print(\"   - More stable than simple transpose for non-square matrices\")\n",
    "    print(\"   - Better reconstruction when encoder weights are not full rank\")\n",
    "    print(\"   - Minimizes reconstruction error in least squares sense\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Hyperparameters\n",
    "    BATCH_SIZE = 250\n",
    "    LATENT_SIZE = 128\n",
    "    NUM_EPOCHS = 40\n",
    "\n",
    "    # Load datasets\n",
    "    print(\"Loading datasets...\")\n",
    "    train_dataset = MNISTDataset('mnist_train.csv')\n",
    "    test_dataset = MNISTDataset('mnist_test.csv')\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    print(\"Datasets loaded successfully\")\n",
    "\n",
    "    # Initialize and test pseudoinverse autoencoder\n",
    "    print(\"\\nTesting Pseudoinverse Autoencoder:\")\n",
    "    model = PseudoinverseAutoencoder(input_size=784, latent_size=LATENT_SIZE)\n",
    "    model.count_parameters()\n",
    "\n",
    "    explain_pseudoinverse_architecture()\n",
    "\n",
    "    # Train the model\n",
    "    print(\"\\nTraining Pseudoinverse Autoencoder...\")\n",
    "    train_model(model, train_loader, NUM_EPOCHS)\n",
    "\n",
    "    # Visualize results\n",
    "    print(\"Generating visualizations...\")\n",
    "    visualize_reconstructions(model, test_loader)\n",
    "\n",
    "    print(\"Training completed!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
