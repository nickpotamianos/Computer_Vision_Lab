{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from autoencoder_mnist_4 import MNISTDataset\n",
    "\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim=784, hidden_dims=[512, 256], latent_dim=2):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        # Encoder layers\n",
    "        encoder_layers = []\n",
    "        in_dim = input_dim\n",
    "        for hidden_dim in hidden_dims:\n",
    "            encoder_layers.extend([\n",
    "                nn.Linear(in_dim, hidden_dim, bias=False),\n",
    "                nn.ReLU()\n",
    "            ])\n",
    "            in_dim = hidden_dim\n",
    "        self.encoder = nn.Sequential(*encoder_layers)\n",
    "\n",
    "        # Mean and variance layers\n",
    "        self.fc_mu = nn.Linear(hidden_dims[-1], latent_dim, bias=False)\n",
    "        self.fc_var = nn.Linear(hidden_dims[-1], latent_dim, bias=False)\n",
    "\n",
    "        # Decoder layers\n",
    "        decoder_layers = []\n",
    "        hidden_dims.reverse()\n",
    "        in_dim = latent_dim\n",
    "        for hidden_dim in hidden_dims:\n",
    "            decoder_layers.extend([\n",
    "                nn.Linear(in_dim, hidden_dim, bias=False),\n",
    "                nn.ReLU()\n",
    "            ])\n",
    "            in_dim = hidden_dim\n",
    "        decoder_layers.extend([\n",
    "            nn.Linear(hidden_dims[-1], input_dim, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        ])\n",
    "        self.decoder = nn.Sequential(*decoder_layers)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        return self.fc_mu(h), self.fc_var(h)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "\n",
    "\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    # Binary Cross Entropy loss\n",
    "    BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "\n",
    "    # KL Divergence loss\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    return BCE + KLD\n",
    "\n",
    "\n",
    "def train_epoch(model, train_loader, optimizer, device, epoch, fixed_noise):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        loss = loss_function(recon_batch, data, mu, logvar)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    # Generate samples from fixed noise at specified epochs\n",
    "    if epoch in [1, 50, 100]:\n",
    "        visualize_fixed_noise_samples(model, fixed_noise, device, epoch)\n",
    "\n",
    "    return train_loss / len(train_loader.dataset)\n",
    "\n",
    "\n",
    "def visualize_fixed_noise_samples(model, fixed_noise, device, epoch):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        samples = model.decode(fixed_noise).cpu()\n",
    "\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        for i in range(64):\n",
    "            plt.subplot(8, 8, i + 1)\n",
    "            plt.imshow(samples[i].reshape(28, 28), cmap='gray')\n",
    "            plt.axis('off')\n",
    "        plt.suptitle(f'Fixed Noise Samples - Epoch {epoch}')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def visualize_latent_space(model, test_loader, device):\n",
    "    model.eval()\n",
    "    z_points = []\n",
    "    labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, y in test_loader:\n",
    "            data = data.to(device)\n",
    "            mu, _ = model.encode(data)\n",
    "            z_points.append(mu.cpu().numpy())\n",
    "            labels.append(y.numpy())\n",
    "\n",
    "    z_points = np.concatenate(z_points, axis=0)\n",
    "    labels = np.concatenate(labels, axis=0)\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    scatter = plt.scatter(z_points[:, 0], z_points[:, 1], c=labels, cmap='tab10')\n",
    "    plt.colorbar(scatter)\n",
    "    plt.title('Latent Space Visualization')\n",
    "    plt.xlabel('z[0]')\n",
    "    plt.ylabel('z[1]')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Hyperparameters as specified in the exercise\n",
    "    batch_size = 250\n",
    "    epochs = 100\n",
    "    latent_dim = 2\n",
    "    learning_rate = 1e-3\n",
    "\n",
    "    # Device configuration\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Load datasets\n",
    "    train_dataset = MNISTDataset('mnist_train.csv')\n",
    "    test_dataset = MNISTDataset('mnist_test.csv')\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Initialize model and optimizer\n",
    "    model = VAE(latent_dim=latent_dim).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Generate fixed noise for visualization\n",
    "    fixed_noise = torch.randn(64, latent_dim).to(device)\n",
    "\n",
    "    # Training loop\n",
    "    train_losses = []\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        loss = train_epoch(model, train_loader, optimizer, device, epoch, fixed_noise)\n",
    "        train_losses.append(loss)\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(f'Epoch {epoch}: Average loss = {loss:.4f}')\n",
    "\n",
    "    # Final visualizations\n",
    "    visualize_latent_space(model, test_loader, device)\n",
    "\n",
    "    # Plot training loss\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_losses)\n",
    "    plt.title('Training Loss Over Time')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
